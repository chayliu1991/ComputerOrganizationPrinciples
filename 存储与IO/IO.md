## 理解存储器的层次结构

### SRAM

SRAM被称为静态存储器：

- 只要是处于通电状态，里面的数据就可以保持存在，而一旦断电，里面的数据就会消失。
- SRAM 的存储密度不高，一个比特的数据需要6~8个晶体管，同样的物理空间下，能够存储的数据有限。
- SRAM 的电路简单，访问速度快

#### CPU的三级缓存

L1：每个CPU核心都有一块属于自己的L1高速缓存，包括指令缓存和数据缓存。L1 的cache往往就嵌在cpu核心的内部。

L2：L2 的 cache 同样是每个CPU核心都有的，不过它往往不在CPU核心的内部，所以L2 cache 的访问速度相对于L1要慢一些

L3：通常是多个CPU核心共用的，尺寸会大一些，访问速度自然慢一些。

### DRAM

内存用的芯片和 Cache 有所不同，它用的是一种叫作 DRAM（Dynamic Random Access Memory，动态随机存取存储器）的芯片，比起 SRAM 来说，它的密度更高，有更大的容量，而且它也比 SRAM 芯片便宜不少。

DRAM 被称为“动态”存储器，是因为 DRAM 需要靠不断地“刷新”，才能保持数据被存储起来。DRAM 的数据访问电路和刷新电路都比 SRAM 更复杂，所以访问延时也就更长。

### 存储器的层级结构

![](../img/s_dram.png)

整个存储器的层次结构，其实都类似于 SRAM 和 DRAM 在性能和价格上的差异。SRAM 更贵，速度更快。DRAM 更便宜，容量更大。SRAM 好像我们的大脑中的记忆，而 DRAM 就好像属于我们自己的书桌。

对于内存来说，SSD（Solid-state drive 或 Solid-state disk，固态硬盘）、HDD（Hard Disk Drive，硬盘）这些被称为硬盘的外部存储设备，就是公共图书馆。

![](../img/mem_hierarchy.png)

CPU 并不是直接和每一种存储器设备打交道，而是每一种存储器设备，只和它相邻的存储设备打交道。比如，CPU Cache 是从内存里加载而来的，或者需要写回内存，并不会直接写回数据到硬盘，也不会直接从硬盘加载数据到 CPU Cache 中，而是先加载到内存，再从内存加载到 Cache 中。
这样，各个存储器只和相邻的一层存储器打交道，并且随着一层层向下，存储器的容量逐层增大，访问速度逐层变慢，而单位存储成本也逐层下降，也就构成了我们日常所说的存储器层次结构。

## 局部性原理

平时进行服务端软件开发的时候，我们通常会把数据存储在数据库里。而服务端系统遇到的第一个性能瓶颈，往往就发生在访问数据库的时候。这个时候，大部分工程师和架构师会拿出一种叫作“缓存”的武器，通过使用Redis或者Memcache这样的开源软件，在数据库前面提供一层缓存的数据，来缓解数据库面临的压力，提升服务端的程序性能。

![](../img/mem_cache.png)

局部性原理（Principle of Locality）包括：

- 时间局部性（temporal locality）如果一个数据被访问了，那么它在短时间内还会被再次访问。
- 空间局部性（spatial locality）如果一个数据被访问了，那么和它相邻的数据也很快会被访问。

![](../img/localiy.png)

![](../img/locality_2.png)

## 高速缓存

在95%的情况下，CPU都只需要访问L1-L3 Cache，从里面读取指令和数据，而无需访问内存。

CPU 从内存中读取数据到CPU cache的过程中，是一小块一小块的读取数据，而不是将整个数组元素读取进来，这样一小块一小块的数据在CPU cache 里面称为 cache line。日常使用的Intel服务器或者PC里，Cache的大小通常是64K字节。

CPU 会首先访问cache，只有当CPU在cache中找不到数据时，才去访问内存。而cache 的速度远远快于内存，这样CPU在等待访问内存的时间就大大缩短了。根据时间局部性原理，刚刚被访问的数据会很快再次被访问。

![](../img/cache_line.png)

对于读取内存中的数据，首先拿到的是说句所在内存块的地址，而直接映射cache的策略，就是确保任何一个内存块的地址，始终映射到一个固定的CPU cache 地址。

### 映射关系

比如说，我们的主内存被分成0～31号这样32个块、我们一共有8个缓存块。用户想要访问第21号内存块，如果21号内存块内容在缓存块中的话，它一定在5号缓存块（21 mod 8 = 5）中：

![](../img/cache_mapping.png)

一种计算方法是比如在这里有8个缓存块，也就是2的3次方，那么在对21取模时，可以对21的2进制表示的数据取低地址的三位，也就是 101，对应的是5：

![](../img/low3.png)

存在的问题：除了12号内存块外，13号和5号等多个内存块的数据都对应5号缓存块，所以取得5号缓存块的数据后并不知道具体是哪个内存块的数据。

### Cache的数据结构

### 组标记

组标记会记录当前缓存块内存储的数据对应的内存块，，而缓存块本身的地址表示访问地址的低N位。比如上面的例子中，21的低3位是 101，缓存块本身的地址已经涵盖了对应的信息，对应的标记，我们只需要记录21的高2位信息，也就是10就可以了。

#### 有效位

用来标记对应缓存块中的数据是否是有效的，如果有效位是0，无论其中的组标记和Cache line里的内容是什么，CPU都不会管这些数据，而要直接访问内存，重新加载数据。

#### 偏移量

CPU在读取数据时，并不是要读取一整个block，而是读取一个它需要的数据，这样的数据在CPU里称为一个 word，具体是哪个字，就用这个字在整个 block里的位置来决定，这个位置称为偏移量。

#### 一个内存的访问地址

索引 +  组标记 + 数据：

- 高位表示组标记
- 低位表示表的索引，以及在对应的 data block 中定位对应字位置的偏移量

#### 内存地址对应到cache里数据结构

内存地址对应到cache里的数据结构：索引 +  有效位 + 组标记 + 数据

如果内存中数据已经在CPU cache 里：

- 根据内存地址的低位，计算在cache中的索引
- 判断有效位，确认cache中的数据是有效的
- 对比内存访问地址的高位，和cache中的组标记，确认cache中的数据就是我们要访问的内存数据
- 从cache line 中读取对应的数据块
- 根据内存地址的偏移量从 cache block 中读取希望读取的字

## CPU高速缓存的写入

![](../img/cpu_cache.png)

我们现在用的Intel CPU，通常都是多核的的。每一个CPU核里面，都有独立属于自己的L1、L2的Cache，然后再有多个CPU核共用的L3的Cache、主内存。

因为CPU Cache的访问速度要比主内存快很多，而在CPU Cache里面，L1/L2的Cache也要比L3的Cache快。CPU始终都是尽可能地从CPU Cache中去获取数据，而不是每一次都要从主内存里面去读取数据。

### 写入策略：写直达

![](../img/write_logic.png)

最简单的一种写入策略，叫作写直达（Write-Through）。在这个策略里，每一次数据都要写入到主内存里面。在写直达的策略里面：

- 写入前，我们会先去判断数据是否已经在Cache里面了。如果数据已经在Cache里面了，我们先把数据写入更新到Cache里面，再写入到主内存里面
- 如果数据不在Cache里，我们就只更新主内存

写直达的这个策略很直观，但是问题也很明显，那就是这个策略很慢。无论数据是不是在Cache里面，我们都需要把数据写到主内存里面。

### 写回

![](../img/write_back.png)

在CPU Cache的写入策略里，还有一种策略就叫作写回（Write-Back）。这个策略里，我们不再是每次都把数据写入到主内存，而是只写到CPU Cache里。只有当CPU Cache里面的数据要被“替换”的时候，我们才把数据写入到主内存里面去。写回策略的过程是这样的：

- 如果发现我们要写入的数据，就在CPU Cache里面，那么我们就只是更新CPU Cache里面的数据。同时，我们会标记CPU Cache里的这个Block是脏（Dirty）的。所谓脏的，就是指这个时候，我们的CPU Cache里面的这个Block的数据，和主内存是不一致的。
- 如果我们发现，我们要写入的数据所对应的Cache Block里，放的是别的内存地址的数据，那么我们就要看一看，那个Cache Block里面的数据有没有被标记成脏的。如果是脏的话，我们要先把这个Cache Block里面的数据，写入到主内存里面。再把当前要写入的数据，写入到Cache里，同时把Cache Block标记成脏的。
- 如果Block里面的数据没有被标记成脏的，那么我们直接把数据写入到Cache里面，然后再把CacheBlock标记成脏的就好了。在用了写回这个策略之后，我们在加载内存数据到Cache里面的时候，也要多出一步同步脏Cache的动作。
- 如果加载内存里面的数据到Cache的时候，发现Cache Block里面有脏标记，我们也要先把Cache Block里的数据写回到主内存，才能加载数据覆盖掉Cache。

## 缓存一致性

比方说，iPhone降价了，我们要把iPhone最新的价格更新到内存里。为了性能问题，它采用了上一讲我们说的写回策略：

![](../img/2cpu_cache.png)

- 先把数据写入到L2 Cache里面，然后把Cache Block标记成脏的。这个时候，数据其实并没有被同步到L3 Cache或者主内存里
- 1号核心希望在这个Cache Block要被交换出去的时候，数据才写入到主内存里。如果我们的CPU只有1号核心这一个CPU核，那这其实是没有问题的。不过，我们旁边还有一个2号核心呢！
- 这个时候，2号核心尝试从内存里面去读取iPhone的价格，结果读到的是一个错误的价格。这是因为，iPhone的价格刚刚被1号核心更新过。但是这个更新的信息，只出现在1号核心的L2 Cache里，而没有出现在2号核心的L2 Cache或者主内存里面。这个问题，就是所谓的缓存一致性问题，1号核心和2号核心的缓存，在这个时候是不一致的。

### 写传播

为了解决这个缓存不一致的问题，我们就需要有一种机制，来同步两个不同核心里面的缓存数据。那这样的机制需要满足什么条件呢？我觉得能够做到下面两点就是合理的。

第一点叫写传播（Write Propagation）。写传播是说，在一个CPU核心里，我们的Cache数据更新，必须能够传播到其他的对应节点的Cache Line里。

### 事务的串行化

第二点叫 事务的串行化（Transaction Serialization），事务串行化是说，我们在一个CPU核心里面的读取和写入，在其他的节点看起来，顺序是一样的。

这一次，我们找一个有4个核心的CPU。1号核心呢，先把iPhone的价格改成了6000块。差不多在同一个时间，2号核心把iPhone的价格改成了5000块。这里两个修改，都会传播到3号核心和4号核心差不多在同一个时间。

![](../img/4cpu_cache.png)

然而这里有个问题，3号核心先收到了2号核心的写传播，再收到1号核心的写传播。所以3号核心看到的iPhone价格是先变成了6000块，再变成了5000块。而4号核心呢，是反过来的，先看到变成了5000块，再变成6000块。虽然写传播是做到了，但是各个Cache里面的数据，是不一致的。

事实上，我们需要的是，从1号到4号核心，都能看到相同顺序的数据变化。比如说，都是先变成了5000块，再变成了6000块。这样，我们才能称之为实现了事务的串行化。

事务的串行化，不仅仅是缓存一致性中所必须的。比如，我们平时所用到的系统当中，最需要保障事务串行的就是数据库。多个不同的连接去访问数据库的时候，化我们必须保障事务的串行化，做不到事务的串行化的数据库，根本没法作为可靠的商业数据库来使用。



